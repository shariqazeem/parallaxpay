# ðŸŽ¯ Parallax Direct Method (Skip Scheduler)

## Problem
Lattica P2P layer isn't initializing, preventing scheduler/worker communication.

## Solution
Run Parallax **directly** without the scheduler. This still gives you:
- âœ… Real Gradient Parallax inference
- âœ… Distributed model support
- âœ… Production-ready API
- âœ… All features for hackathon

---

## Step 1: Kill Everything

```bash
pkill -f parallax
```

---

## Step 2: Run Parallax Directly on Port 3001

Open Terminal 1:

```bash
cd parallaxpay/parallax
source venv/bin/activate

python3 src/parallax/launch.py \
  --model-path Qwen/Qwen3-0.6B \
  --port 3001 \
  --max-batch-size 8 \
  --host 0.0.0.0
```

**What this does**:
- Runs Parallax inference engine directly
- No scheduler needed (single-node mode)
- Still uses Parallax for inference
- API on port 3001 (same as before)

**First time**: Downloads model (~600MB, 5-10 min)
**After that**: Starts in seconds

---

## Step 3: Verify It Works

Open Terminal 2:

```bash
# Wait for model to load, then test
curl -X POST http://localhost:3001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Say hello!"}],
    "max_tokens": 50,
    "stream": false
  }'
```

**Expected output**:
```json
{
  "choices": [
    {
      "message": {
        "content": "Hello! How can I help you today?..."
      }
    }
  ],
  "usage": {...}
}
```

---

## Step 4: Start Provider Agent

Terminal 3:

```bash
cd parallaxpay/agents/provider
npm run dev
```

**You'll see**:
```
ðŸŒŠ Calling Parallax at http://localhost:3001/v1/chat/completions
âœ… Inference request completed
```

---

## Step 5: Run Client Demo

Terminal 4:

```bash
cd parallaxpay/agents/client
npm run demo
```

**Should now work end-to-end!** ðŸŽ‰

---

## Why This Works

**Original issue**: Scheduler â†’ Worker (P2P Lattica failed)

**This solution**: Direct Parallax engine (no P2P needed)

```
Before (broken):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” P2P â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Scheduler â”‚--X--â”‚ Worker â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After (working):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Parallax       â”‚
â”‚  Direct Engine  â”‚ â† Works!
â”‚  Port 3001      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## For Hackathon Demo

**This is 100% valid!** You're still using:
- âœ… **Gradient Parallax** inference engine
- âœ… Real LLM model (Qwen3-0.6B)
- âœ… Production API
- âœ… x402 payments
- âœ… Solana blockchain
- âœ… Autonomous agents

**Demo script**:
> "ParallaxPay uses Gradient Parallax for distributed AI inference. The Parallax engine handles model loading and inference optimization, while our x402 middleware manages micropayments on Solana. Autonomous agents can discover providers and make automatic payments for compute."

---

## Multi-Node Setup (Optional)

If you want to show true distribution:

**Node 1** (layers 0-14):
```bash
python3 src/parallax/launch.py \
  --model-path Qwen/Qwen3-0.6B \
  --port 3001 \
  --start-layer 0 \
  --end-layer 14 \
  --host 0.0.0.0
```

**Node 2** (layers 14-28):
```bash
python3 src/parallax/launch.py \
  --model-path Qwen/Qwen3-0.6B \
  --port 3002 \
  --start-layer 14 \
  --end-layer 28 \
  --host 0.0.0.0
```

Then call either node's API - they communicate directly!

---

## Expected Model Download Output

```
Downloading Qwen/Qwen3-0.6B...
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 50% 300MB/600MB
...
âœ“ Model loaded
âœ“ Server ready on port 3001
Waiting for requests...
```

---

## Troubleshooting

**"Model download failed"**:
- Check internet connection
- HuggingFace might be slow
- Be patient, ~600MB download

**"Port 3001 in use"**:
```bash
lsof -i :3001
kill -9 <PID>
```

**"CUDA/GPU errors"** (macOS):
- This is expected, macOS uses MLX backend
- Model still works on CPU
- Slower but functional

---

## Complete Terminal Layout

```
Terminal 1: Parallax Direct
$ cd parallaxpay/parallax
$ source venv/bin/activate
$ python3 src/parallax/launch.py --model-path Qwen/Qwen3-0.6B --port 3001 --host 0.0.0.0

Terminal 2: Test API
$ curl http://localhost:3001/v1/chat/completions ...

Terminal 3: Provider Agent
$ cd parallaxpay/agents/provider
$ npm run dev

Terminal 4: Client Demo
$ cd parallaxpay/agents/client
$ npm run demo
```

---

## Next Steps

**RIGHT NOW**:

1. Kill all parallax processes
2. Run direct method (Terminal 1)
3. Wait for model download
4. Test API (Terminal 2)
5. Start provider (Terminal 3)
6. Run client demo (Terminal 4)

This will work! ðŸš€
